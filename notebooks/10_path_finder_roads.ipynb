{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "from configobj import ConfigObj\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from functions.roads import download_roads, roads_to_points\n",
    "from functions.utils import database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/home/jovyan/shared/rural_planner_refactoring/config_files/config_pe\"\n",
    "\n",
    "parser = ConfigObj(config_path)\n",
    "\n",
    "sql_path = parser['sql_path']\n",
    "country_folder = parser['country_folder']\n",
    "\n",
    "road_types = dict(parser['path_finder_roads_params']['road_types'])\n",
    "output_path = parser['path_finder_roads_params']['output_path']\n",
    "output_filename = parser['path_finder_roads_params']['output_filename']\n",
    "radius = int(parser['path_finder_roads_params']['max_road_radius'])\n",
    "distance = int(parser['path_finder_roads_params']['points_road_distance'])\n",
    "threshold_distance = int(parser['path_finder_roads_params']['threshold_distance'])\n",
    "penalty = int(parser['path_finder_roads_params']['penalty'])\n",
    "\n",
    "schema = parser['path_finder_roads_params']['schema']\n",
    "roads_table_intermediate = parser['path_finder_roads_params']['roads_table']\n",
    "roads_table = parser['path_finder_roads_params']['roads_table_dump']\n",
    "table_roads_points = parser['path_finder_roads_params']['roads_points_table']\n",
    "table_intersections = parser['path_finder_roads_params']['table_intersections']\n",
    "table_clusters = parser['clustering_params']['output_table']\n",
    "table_towers = parser['transport_by_tower_params']['table_infrastructure']\n",
    "auxiliary_table = parser['path_finder_roads_params']['auxiliary_table']\n",
    "table_cluster_points = parser['path_finder_roads_params']['table_cluster_points']\n",
    "table_nodes_roads = parser['path_finder_roads_params']['table_nodes_roads']\n",
    "table_clusters_links = parser['path_finder_roads_params']['table_clusters_links']\n",
    "table_edges_roads = parser['path_finder_roads_params']['table_edges_roads']\n",
    "table_node_replacement_map = parser['path_finder_roads_params']['table_node_replacement_map']\n",
    "table_cluster_node_map = parser['path_finder_roads_params']['table_cluster_node_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GeoDataFrame' object has no attribute 'replace_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-016cb536338b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mroads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroads_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mroads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mroads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'road_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GeoDataFrame' object has no attribute 'replace_index'"
     ]
    }
   ],
   "source": [
    "## Download national and departamental roads if not downloded, unzip, read SHP and upload to DB\n",
    "roads = pd.DataFrame()\n",
    "for road_type in (road_types.keys()):\n",
    "    roads_n = download_roads(parser, road_type)\n",
    "    roads = roads.append(roads_n)\n",
    "\n",
    "roads.reset_index(inplace=True)\n",
    "roads['road_id'] = roads.index\n",
    "\n",
    "with database(parser) as db:\n",
    "    roads.to_sql(roads_table_intermediate, con=db, if_exists = 'replace', schema = schema, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the road table turning multilinestring to linestring.\n",
    "#This is needed to have a homogeneuous data type throughout all roads data set.\n",
    "#It requires having the roads data set loaded into the database\n",
    "\n",
    "query_path = sql_path + '/' + country_folder + '/' + 'path_finder_roads_geom.sql'\n",
    "query_path_dump = sql_path + '/' + country_folder + '/' + 'path_finder_roads_dump_lines.sql'\n",
    "\n",
    "with open(query_path) as file, open(query_path_dump) as file_dump, database(parser) as db:\n",
    "    query = file.read()\n",
    "    query_formatted = query.format(schema = schema, table_roads = roads_table_intermediate)\n",
    "    db.execute(query_formatted)\n",
    "    \n",
    "    query = file_dump.read()\n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_roads = roads_table_intermediate,\n",
    "                                   table_roads_linestring = roads_table)\n",
    "    db.execute(query_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table with points instead of linestring separated by 1km. Remove duplicates and upload to DB (creating geometry indexes)\n",
    "#Takes less than 1.5 hours\n",
    "roads_points = roads_to_points(parser, distance)\n",
    "roads_points = roads_points.drop_duplicates()\n",
    "\n",
    "query_path = sql_path + '/' + country_folder + '/' + 'path_finder_roads_points_idx.sql'\n",
    "\n",
    "with open(query_path) as file, database(parser) as db:\n",
    "    roads_points.to_sql(table_roads_points, con=engine, if_exists = 'replace', schema = schema, index = False)\n",
    "    query = file.read()\n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_roads_points = table_roads_points)\n",
    "    db.execute(query_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the table with the intersections between roads\n",
    "\n",
    "query_path = sql_path + '/' + country_folder + '/' + 'path_finder_roads_intersections.sql'\n",
    "\n",
    "with open(query_path) as file, database(parser) as db:\n",
    "    query = file.read()\n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_roads_points = table_roads_points,\n",
    "                                   table_intersections = table_intersections)\n",
    "    db.execute(query_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create auxiliary table with all points to connect by fiber or terminal nodes (all cluster centroids + towers)\n",
    "#Assign to them nearest road point \n",
    "#Create road node table based on near clusters' weight, add intersections and borders\n",
    "#Create final node table output\n",
    "\n",
    "query_path = sql_path + '/' + country_folder + '/' + 'path_finder_roads_clusters.sql'\n",
    "query_path_assignation = sql_path + '/' + country_folder + '/' + 'path_finder_roads_nearest_cluster.sql'\n",
    "query_path_roads = sql_path + '/' + country_folder + '/' + 'path_finder_roads_cluster_nodes.sql'\n",
    "query_path_borders = sql_path + '/' + country_folder + '/' + 'path_finder_roads_borders.sql'\n",
    "query_path_final = sql_path + '/' + country_folder + '/' + 'path_finder_roads_final_node_table.sql'\n",
    "\n",
    "with open(query_path) as file, open(query_path_assignation) as file_assignation, open(query_path_roads) as file_roads, open(query_path_borders) as file_borders, open(query_path_final) as file_final, database(parser) as db:\n",
    "    query = file.read()\n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_clusters = table_clusters,\n",
    "                                   auxiliary_table = auxiliary_table,\n",
    "                                   table_towers = table_towers)\n",
    "    db.execute(query_formatted)\n",
    "    \n",
    "    query = file_assignation.read()\n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_cluster_points = table_cluster_points,\n",
    "                                   table_roads_points = table_roads_points,\n",
    "                                   auxiliary_table = auxiliary_table,\n",
    "                                   radius = radius)\n",
    "    db.execute(query_formatted)\n",
    "    \n",
    "    query = file_roads.read()\n",
    "    \n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_cluster_points = table_cluster_points,\n",
    "                                   table_roads_points = table_roads_points,\n",
    "                                   table_clusters_links = table_clusters_links,\n",
    "                                   auxiliary_table = auxiliary_table,\n",
    "                                   penalty = penalty,\n",
    "                                   threshold_distance = threshold_distance,\n",
    "                                   table_nodes_roads = table_nodes_roads,\n",
    "                                   table_intersections = table_intersections)\n",
    "    db.execute(query_formatted)\n",
    "    \n",
    "    query = file_final.read()\n",
    "    \n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_cluster_points = table_cluster_points,\n",
    "                                   table_intersections = table_intersections,\n",
    "                                   table_roads_points = table_roads_points,\n",
    "                                   table_clusters_links = table_clusters_links,\n",
    "                                   table_nodes_roads = table_nodes_roads)\n",
    "    db.execute(query_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import node table\n",
    "\n",
    "query_path = sql_path + '/' + country_folder + '/' + 'path_finder_roads_import_nodes.sql'\n",
    "\n",
    "with open(query_path) as file, database(parser) as db:\n",
    "    query = file.read()\n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_nodes_roads = table_nodes_roads)\n",
    "    df_nodes = pd.read_sql_query(query_formatted, db)\n",
    "    \n",
    "# Create edge table by linking each node to the immidiately precedent and following nodes within the same stretch (one edge per direction)\n",
    "df_edges_reverse = pd.DataFrame(columns = ['node_1', 'node_2', 'weight'])\n",
    "\n",
    "df_edges.node_1 = df_nodes.node_id\n",
    "df_edges.node_2 = df_nodes.groupby('stretch_id').shift(-1)['node_id']\n",
    "df_edges.weight =  abs(df_nodes.groupby('stretch_id').shift(-1)['division'] - df_nodes['division'])*df_nodes['stretch_length']/1000\n",
    "\n",
    "df_edges_reverse.node_1 = df_nodes.groupby('stretch_id').shift(-1)['node_id']\n",
    "df_edges_reverse.node_2 = df_nodes.node_id\n",
    "df_edges_reverse.weight =  abs(df_nodes['division'] - df_nodes.groupby('stretch_id').shift(-1)['division'] )*df_nodes['stretch_length']/1000\n",
    "\n",
    "df_edges = df_edges.append(df_edges_reverse)\n",
    "df_edges = df_edges.ix[(~np.isnan(df_edges['node_2'])& ~np.isnan(df_edges['node_1']))]\n",
    "\n",
    "#Import intersections and append to edges table\n",
    "\n",
    "query_path = sql_path + '/' + country_folder + '/' + 'path_finder_roads_import_intersections.sql'\n",
    "\n",
    "with open(query_path) as file, database(parser) as db:\n",
    "    query = file.read()\n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_intersections = table_intersections,\n",
    "                                   table_nodes_roads = table_nodes_roads)\n",
    "    df_intersections = pd.read_sql_query(query_formatted, db)\n",
    "\n",
    "df_edges = df_edges.append(df_intersections)\n",
    "\n",
    "df_edges['node_1'] = df_edges['node_1'].astype('int64')\n",
    "df_edges['node_2'] = df_edges['node_2'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate edges and merge them in one single edge (we keep the one with maximum weight)\n",
    "\n",
    "##Code to simplify intersections\n",
    "#Select half of the edges with a weight smaller than a threshold (this threshold has to be the same as the used in the intersection definition/2)\n",
    "#Take into account only one direction\n",
    "threshold = 0.5/2/10\n",
    "\n",
    "#We create a mapping between nodes that we want to unify (node_1 --> node_2). To clean complex dependencies (1-->2, 2-->3, 3-->7 needs to become (1-->2, 1-->3, 1-->7))\n",
    "df_edges_zero_weight = df_edges.loc[(abs(df_edges['weight']) <= threshold) & (df_edges['node_1'] < df_edges['node_2'])].reset_index().drop('index', axis = 1)\n",
    "\n",
    "df_edges_zero_weight.drop(columns='weight', inplace=True)\n",
    "d1 = OrderedDict({row['node_2']:row['node_1'] for _,row in df_edges_zero_weight.iterrows()})\n",
    "\n",
    "while any(x in list(d1.values()) for x in list(d1.keys())):\n",
    "    for k, v in (d1.items()):\n",
    "        if (v in d1.keys()):\n",
    "            d1[k] = d1[v]\n",
    "            \n",
    "df_edges.node_1 = df_edges.node_1.map(d1).fillna(df_edges['node_1'])\n",
    "df_edges.node_2 = df_edges.node_2.map(d1).fillna(df_edges['node_2'])\n",
    "\n",
    "#Remove the edges of weight zero and remove duplicates, keeping the ones with higher weight\n",
    "df_edges = df_edges.ix[abs(df_edges['weight']) >= threshold].sort_values(by = ['node_1', 'weight'], ascending = [True, False]).drop_duplicates(['node_1', 'node_2'])\n",
    "\n",
    "#Replace nodes that have been altered and update the weight of replacement nodes with the sum of all the nodes that it is replacing\n",
    "df_nodes_update = df_nodes.copy(deep=True)\n",
    "df_nodes_update.node_id = df_nodes_update.node_id.map(d1).fillna(df_nodes_update['node_id'])\n",
    "df_nodes_update = df_nodes_update.sort_values('node_id').groupby('node_id').agg({'stretch_id': lambda x: x.iloc[0],\n",
    "                                                               'division': lambda x: x.iloc[0],\n",
    "                                                               'score': lambda x: x.iloc[0],\n",
    "                                                               'cluster_weight':sum,\n",
    "                                                               'node_weight':sum,\n",
    "                                                               'stretch_length': lambda x: x.iloc[0],\n",
    "                                                                'geom': lambda x: x.iloc[0]}).reset_index('node_id')\n",
    "\n",
    "# Upload to database and update nodes in node cluster map table\n",
    "query_path = sql_path + '/' + country_folder + '/' + 'path_finder_roads_update_node_map.sql'\n",
    "\n",
    "with open(query_path) as file, database(parser) as db:\n",
    "    \n",
    "    df_nodes_update.to_sql(table_nodes_roads, con=db, if_exists = 'replace', schema = schema, index = False)\n",
    "    df_edges_zero_weight.to_sql(table_node_replacement_map, con=db, if_exists = 'replace', schema = schema, index = False)\n",
    "    \n",
    "    query = file.read()\n",
    "    query_formatted = query.format(schema = schema,\n",
    "                                   table_node_replacement_map = table_node_replacement_map,\n",
    "                                   table_cluster_node_map = table_cluster_node_map,\n",
    "                                   auxiliary_table = auxiliary_table,\n",
    "                                   table_nodes_roads = table_nodes_roads)\n",
    "    db.execute(query_formatted)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export edge table\n",
    "with database(parser) as db:\n",
    "    df_edges.to_sql(table_edges_roads, con=engine, if_exists = 'replace', schema = schema, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
